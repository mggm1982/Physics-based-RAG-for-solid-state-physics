{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13497711,"sourceType":"datasetVersion","datasetId":8570014},{"sourceId":13547421,"sourceType":"datasetVersion","datasetId":8603949},{"sourceId":13552533,"sourceType":"datasetVersion","datasetId":8607588},{"sourceId":13552624,"sourceType":"datasetVersion","datasetId":8607662},{"sourceId":13553276,"sourceType":"datasetVersion","datasetId":8608181}],"dockerImageVersionId":31153,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Run once at top of notebook\n!pip install -q -U transformers datasets accelerate peft bitsandbytes safetensors\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T17:22:55.400825Z","iopub.execute_input":"2025-10-30T17:22:55.401089Z","iopub.status.idle":"2025-10-30T17:24:53.262165Z","shell.execute_reply.started":"2025-10-30T17:22:55.401065Z","shell.execute_reply":"2025-10-30T17:24:53.261377Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m99.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m506.8/506.8 kB\u001b[0m \u001b[31m25.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m375.8/375.8 kB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m504.9/504.9 kB\u001b[0m \u001b[31m32.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m59.4/59.4 MB\u001b[0m \u001b[31m30.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m485.8/485.8 kB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m566.1/566.1 kB\u001b[0m \u001b[31m29.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m47.7/47.7 MB\u001b[0m \u001b[31m37.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m86.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m91.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m71.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m40.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m21.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m43.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\npylibcudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\ncudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\nbigframes 2.12.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\nbigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.1.0 which is incompatible.\nlibcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\ngradio 5.38.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.0a1 which is incompatible.\ncudf-polars-cu12 25.6.0 requires pylibcudf-cu12==25.6.*, but you have pylibcudf-cu12 25.2.2 which is incompatible.\npandas-gbq 0.29.2 requires google-api-core<3.0.0,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\npylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import json\nfrom datasets import Dataset\nfrom pathlib import Path\n\nTRAIN_JSON_PATH = \"/kaggle/input/allcombined/Untitled-1.json\"   # <-- change to your file path if different\nMAX_LENGTH = 512   # tokens (adjust if needed)\n\n# Load raw JSON list\nwith open(TRAIN_JSON_PATH, \"r\", encoding=\"utf-8\") as f:\n    raw = json.load(f)\n\n# Normalize / build prompt/response strings\nexamples = []\nfor item in raw:\n    # safeguard field names: uses question & answer fields as user showed\n    q = item.get(\"question\", \"\").strip()\n    a = item.get(\"answer\", \"\").strip()\n    # you can include metadata if you want (source_pdf, page)\n    meta = f\" [source: {item.get('source_pdf','')}, page: {item.get('page','')}]\"\n    # Compose training text â€” you can change format to your instruction style\n    text = f\"### Question:\\n{q}\\n\\n### Answer:\\n{a}{meta}\"\n    examples.append({\"text\": text})\n\nds = Dataset.from_list(examples)\nprint(\"Built dataset with\", len(ds), \"examples\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T07:58:07.348887Z","iopub.execute_input":"2025-10-30T07:58:07.349241Z","iopub.status.idle":"2025-10-30T07:58:11.996530Z","shell.execute_reply.started":"2025-10-30T07:58:07.349214Z","shell.execute_reply":"2025-10-30T07:58:11.995710Z"}},"outputs":[{"name":"stdout","text":"Built dataset with 1504 examples\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"from transformers import AutoTokenizer\nMODEL_NAME = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n# If you saved locally earlier, point to local folder. Otherwise use HF ID.\n\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True, trust_remote_code=True)\n# ensure pad token exists for Trainer\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token\n\ndef tokenize_fn(batch):\n    return tokenizer(batch[\"text\"], truncation=True, max_length=MAX_LENGTH, padding=\"max_length\")\n\ntokenized = ds.map(tokenize_fn, batched=True, remove_columns=[\"text\"])\ntokenized.set_format(type=\"torch\")\nprint(\"Tokenized dataset example:\", tokenized[0][\"input_ids\"].shape)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T07:59:02.090190Z","iopub.execute_input":"2025-10-30T07:59:02.090739Z","iopub.status.idle":"2025-10-30T07:59:09.310464Z","shell.execute_reply.started":"2025-10-30T07:59:02.090715Z","shell.execute_reply":"2025-10-30T07:59:09.309823Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f26247591f4244c6b48766c378caf57e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5431107578a2499b94cfcf005da1ef97"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1504 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a07a58999cad44989a9b64bb8173a3d1"}},"metadata":{}},{"name":"stdout","text":"Tokenized dataset example: torch.Size([512])\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import torch\nfrom transformers import AutoModelForCausalLM\nfrom peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\ndtype = torch.float16 if DEVICE == \"cuda\" else torch.float32\n\nprint(\"Device:\", DEVICE, \"dtype:\", dtype)\n\n# Load base model (no quantization) â€” set device_map=\"auto\" for multi-GPU or bigger\nmodel = AutoModelForCausalLM.from_pretrained(\n    MODEL_NAME,\n    torch_dtype=dtype,\n    device_map=\"auto\"\n)\n\n# Prepare for k-bit training if you use 8/4-bit; if not using bitsandbytes it's safe too.\n# prepare_model_for_kbit_training(model)  # uncomment only if you're using k-bit quantization\n\n# LoRA config\nlora_config = LoraConfig(\n    r=8,                       # rank\n    lora_alpha=32,\n    target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"],  # cover common proj names\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\"\n)\n\nmodel = get_peft_model(model, lora_config)\nmodel.print_trainable_parameters()  # verify only LoRA params are trainable\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T07:59:19.448775Z","iopub.execute_input":"2025-10-30T07:59:19.449590Z","iopub.status.idle":"2025-10-30T08:00:09.794128Z","shell.execute_reply.started":"2025-10-30T07:59:19.449568Z","shell.execute_reply":"2025-10-30T08:00:09.793391Z"}},"outputs":[{"name":"stderr","text":"2025-10-30 07:59:21.627114: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1761811161.857545      37 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1761811161.917325      37 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"Device: cuda dtype: torch.float16\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/679 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0774ae81059140ac8ed7959802801d11"}},"metadata":{}},{"name":"stderr","text":"`torch_dtype` is deprecated! Use `dtype` instead!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/3.55G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2b2db009584d407687c56eef941a91bf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/181 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c37b7bdeb3d94afbb7849e26e2e7f431"}},"metadata":{}},{"name":"stdout","text":"trainable params: 2,179,072 || all params: 1,779,267,072 || trainable%: 0.1225\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"from transformers import TrainingArguments, Trainer, DataCollatorForLanguageModeling\n\nOUTPUT_DIR = \"/kaggle/working/deepseek-1.5b-physics-lora\"\nper_device_batch = 1\ngrad_accum = 8   # effective batch = 8\n\ntraining_args = TrainingArguments(\n    output_dir=OUTPUT_DIR,\n    per_device_train_batch_size=per_device_batch,\n    gradient_accumulation_steps=grad_accum,\n    num_train_epochs=2,\n    learning_rate=2e-4,\n    fp16=(DEVICE==\"cuda\"),\n    logging_steps=50,\n    save_strategy=\"epoch\",\n    save_total_limit=2,\n    remove_unused_columns=False,\n    report_to=\"none\"\n)\n\ndata_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized,\n    data_collator=data_collator,\n    tokenizer=tokenizer,\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T08:00:41.251212Z","iopub.execute_input":"2025-10-30T08:00:41.251868Z","iopub.status.idle":"2025-10-30T08:00:42.474158Z","shell.execute_reply.started":"2025-10-30T08:00:41.251844Z","shell.execute_reply":"2025-10-30T08:00:42.473327Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_37/2494954169.py:23: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# Train! Might take minutes â†’ hours depending on dataset size and epochs\ntrainer.train()\n# Save final\ntrainer.save_model(OUTPUT_DIR)\ntokenizer.save_pretrained(OUTPUT_DIR)\nprint(\"âœ… Fine-tuned LoRA adapters saved to:\", OUTPUT_DIR)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T08:00:56.203114Z","iopub.execute_input":"2025-10-30T08:00:56.203407Z","iopub.status.idle":"2025-10-30T08:14:21.297059Z","shell.execute_reply.started":"2025-10-30T08:00:56.203388Z","shell.execute_reply":"2025-10-30T08:14:21.296141Z"}},"outputs":[{"name":"stderr","text":"The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 151646, 'pad_token_id': 151643}.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='376' max='376' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [376/376 13:21, Epoch 2/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>50</td>\n      <td>2.918800</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>1.929700</td>\n    </tr>\n    <tr>\n      <td>150</td>\n      <td>1.659200</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>1.559000</td>\n    </tr>\n    <tr>\n      <td>250</td>\n      <td>1.519100</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>1.478300</td>\n    </tr>\n    <tr>\n      <td>350</td>\n      <td>1.470500</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"âœ… Fine-tuned LoRA adapters saved to: /kaggle/working/deepseek-1.5b-physics-lora\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"!tar -czf /kaggle/working/deepseek-1.5b-physics-lora.tar.gz -C /kaggle/working deepseek-1.5b-physics-lora\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T03:00:02.108974Z","iopub.execute_input":"2025-10-30T03:00:02.109662Z","iopub.status.idle":"2025-10-30T03:00:06.691404Z","shell.execute_reply.started":"2025-10-30T03:00:02.109635Z","shell.execute_reply":"2025-10-30T03:00:06.690633Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"!pip install -q \"transformers>=4.43,<4.46\" \"sentence-transformers>=3.0,<3.2\" \"huggingface_hub>=0.26,<0.28\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T17:25:20.314192Z","iopub.execute_input":"2025-10-30T17:25:20.314688Z","iopub.status.idle":"2025-10-30T17:25:32.341619Z","shell.execute_reply.started":"2025-10-30T17:25:20.314657Z","shell.execute_reply":"2025-10-30T17:25:32.340823Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m9.9/9.9 MB\u001b[0m \u001b[31m73.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m245.3/245.3 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m450.7/450.7 kB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m62.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ngradio 5.38.1 requires huggingface-hub>=0.28.1, but you have huggingface-hub 0.27.1 which is incompatible.\ngradio 5.38.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.0a1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"!pip install -q faiss-cpu pymupdf pillow pytesseract nltk\n\n# download punkt tokenizer for sentence splitting\n\nimport nltk\nnltk.download('punkt')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T17:25:36.245292Z","iopub.execute_input":"2025-10-30T17:25:36.246065Z","iopub.status.idle":"2025-10-30T17:25:44.699695Z","shell.execute_reply.started":"2025-10-30T17:25:36.246036Z","shell.execute_reply":"2025-10-30T17:25:44.698997Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m31.4/31.4 MB\u001b[0m \u001b[31m56.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.1/24.1 MB\u001b[0m \u001b[31m71.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h","output_type":"stream"},{"name":"stderr","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n","output_type":"stream"},{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"import os\nimport io\nimport time\nimport math\nimport json\nimport faiss\nimport fitz            # PyMuPDF\nimport torch\nimport pytesseract\nfrom PIL import Image\nfrom pathlib import Path\nfrom typing import List, Tuple\nfrom sentence_transformers import SentenceTransformer\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig\nimport transformers.utils.hub as hf_hub_utils","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T17:25:49.701945Z","iopub.execute_input":"2025-10-30T17:25:49.702648Z","iopub.status.idle":"2025-10-30T17:26:21.706137Z","shell.execute_reply.started":"2025-10-30T17:25:49.702624Z","shell.execute_reply":"2025-10-30T17:26:21.705539Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n  from tqdm.autonotebook import tqdm, trange\n2025-10-30 17:26:04.533327: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1761845164.912372      37 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1761845165.019298      37 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"hf_hub_utils.list_repo_templates = lambda *args, **kwargs: []\n\n# Device\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(\"Device:\", DEVICE)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T17:34:48.150792Z","iopub.execute_input":"2025-10-30T17:34:48.151636Z","iopub.status.idle":"2025-10-30T17:34:48.156393Z","shell.execute_reply.started":"2025-10-30T17:34:48.151605Z","shell.execute_reply":"2025-10-30T17:34:48.155472Z"}},"outputs":[{"name":"stdout","text":"Device: cuda\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"from peft import PeftModel\nBASE_MODEL = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\nLORA_ADAPTER_PATH = \"/kaggle/working/deepseek-1.5b-physics-lora\"  # <-- your fine-tuned adapter path\nMERGED_MODEL_PATH = \"/kaggle/working/deepseek-physics-finetuned\"  # <-- where to save merged model\n\n# 1. Load base model in fp16 if GPU is available\ndtype = torch.float16 if torch.cuda.is_available() else torch.float32\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\nbase_model = AutoModelForCausalLM.from_pretrained(\n    BASE_MODEL,\n    torch_dtype=dtype,\n    device_map=\"auto\"\n)\n\n# 2. Load tokenizer\ntokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, trust_remote_code=True)\n\n# 3. Load LoRA adapter weights\nprint(\"ğŸ”§ Loading LoRA adapter...\")\nlora_model = PeftModel.from_pretrained(base_model, LORA_ADAPTER_PATH)\n\n# 4. Merge LoRA into base weights\nprint(\"ğŸ§  Merging LoRA adapter into base model weights...\")\nmerged_model = lora_model.merge_and_unload()   # <-- this creates a standalone model\n\n# 5. Save merged model and tokenizer\nprint(f\"ğŸ’¾ Saving merged model to {MERGED_MODEL_PATH}\")\nmerged_model.save_pretrained(MERGED_MODEL_PATH)\ntokenizer.save_pretrained(MERGED_MODEL_PATH)\n\nprint(\"âœ… Merged model saved successfully!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T08:20:46.565959Z","iopub.execute_input":"2025-10-30T08:20:46.566855Z","iopub.status.idle":"2025-10-30T08:20:57.746314Z","shell.execute_reply.started":"2025-10-30T08:20:46.566824Z","shell.execute_reply":"2025-10-30T08:20:57.745354Z"}},"outputs":[{"name":"stdout","text":"ğŸ”§ Loading LoRA adapter...\nğŸ§  Merging LoRA adapter into base model weights...\nğŸ’¾ Saving merged model to /kaggle/working/deepseek-physics-finetuned\nâœ… Merged model saved successfully!\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"PDF_INPUT_PATH = \"/kaggle/input/pdf-file/Angew Chem Int Ed - 2023 - Glico - Toward MagnetoOptical Cryogenic Thermometers with High Sensitivity  A Magnetic.pdf\"\n\n# Model choices\nEMB_MODEL_NAME = \"all-mpnet-base-v2\"  # stronger semantic embeddings\nHF_GEN_MODEL = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"  # generator\n\n# Chunking / retrieval params\nCHUNK_SIZE_WORDS = 400\nCHUNK_OVERLAP_WORDS = 80\nTOP_K = 5\n\n# Caching (optional)\nCACHE_DIR = \"/kaggle/working/rag_cache\"\nos.makedirs(CACHE_DIR, exist_ok=True)\nEMB_CACHE_FNAME = os.path.join(CACHE_DIR, \"embeddings.npy\")\nCHUNKS_CACHE_FNAME = os.path.join(CACHE_DIR, \"chunks.json\")\nFAISS_INDEX_FNAME = os.path.join(CACHE_DIR, \"faiss.index\")\nEMB_MODEL_CACHE = os.path.join(CACHE_DIR, \"sbert\")  # optional SBERT cache path\n\n# 3) PDF extraction (text + images OCR + simple \"table-like\" extraction)\n# ============================\nimport re\nfrom nltk.tokenize import sent_tokenize\n\ndef extract_text_from_pdf(pdf_path: str) -> str:\n    \"\"\"\n    Extracts text from a PDF including:\n      - textual content via PyMuPDF\n      - \"table-like\" blocks detected by tabs or multiple consecutive spaces\n      - OCR from embedded images via pytesseract\n    Returns the combined text.\n    \"\"\"\n    doc = fitz.open(pdf_path)\n    page_texts = []\n    for pno, page in enumerate(doc, start=1):\n        # 1) regular text (flow text)\n        text = page.get_text(\"text\") or \"\"\n        \n        # 2) table-like heuristics: detect blocks with multiple tabs / consecutive multi-space\n        blocks = page.get_text(\"blocks\")\n        table_texts = []\n        for b in blocks:\n            block_text = b[4].strip()\n            # heuristics: tabs or many consecutive spaces (possible table or fixed-width)\n            if \"\\t\" in block_text or re.search(r\" {2,}\", block_text):\n                table_texts.append(block_text)\n        if table_texts:\n            text += \"\\n\\n\" + \"\\n\".join(table_texts)\n        \n        # 3) images -> OCR\n        images = page.get_images(full=True)\n        for img_index, img in enumerate(images):\n            xref = img[0]\n            try:\n                base_image = doc.extract_image(xref)\n                image_bytes = base_image[\"image\"]\n                pil = Image.open(io.BytesIO(image_bytes)).convert(\"RGB\")\n                ocr = pytesseract.image_to_string(pil)\n                if ocr and ocr.strip():\n                    text += \"\\n\\n[OCR PAGE %d IMAGE %d]\\n%s\" % (pno, img_index, ocr)\n            except Exception:\n                # continue without crashing if an image fails\n                continue\n        if text.strip():\n            page_texts.append(f\"[PAGE {pno}]\\n\" + text.strip())\n    doc.close()\n    return \"\\n\\n\".join(page_texts)\n\n# Quick check: ensure file exists\nif not os.path.isfile(PDF_INPUT_PATH):\n    raise FileNotFoundError(f\"PDF not found at {PDF_INPUT_PATH}. Run !ls /kaggle/input to check path.\")\nprint(\"PDF found â€” ready to extract.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T18:05:40.722561Z","iopub.execute_input":"2025-10-30T18:05:40.723039Z","iopub.status.idle":"2025-10-30T18:05:40.773128Z","shell.execute_reply.started":"2025-10-30T18:05:40.723004Z","shell.execute_reply":"2025-10-30T18:05:40.772306Z"}},"outputs":[{"name":"stdout","text":"PDF found â€” ready to extract.\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"def chunk_text(text: str, chunk_size_words: int = CHUNK_SIZE_WORDS, overlap_words: int = CHUNK_OVERLAP_WORDS) -> List[str]:\n    \"\"\"\n    Splits text into overlapping chunks based on sentences.\n    - chunk_size_words: approx target words per chunk\n    - overlap_words: words to keep overlapping between consecutive chunks\n    \"\"\"\n    # split into sentences (better than naive \". \")\n    sentences = sent_tokenize(text)\n    chunks = []\n    cur_chunk = []\n    cur_words = 0\n    for sent in sentences:\n        sent_words = len(sent.split())\n        if cur_words + sent_words > chunk_size_words and cur_chunk:\n            # flush\n            chunk_text = \" \".join(cur_chunk).strip()\n            if len(chunk_text.split()) >= 20:\n                chunks.append(chunk_text)\n            # start new chunk with overlap\n            # keep last overlap_words of current chunk as new start\n            words_keep = []\n            accum = 0\n            # iterate sentences in cur_chunk reversed to keep overlap\n            for s in reversed(cur_chunk):\n                w = s.split()\n                if accum + len(w) > overlap_words:\n                    break\n                words_keep.insert(0, s)\n                accum += len(w)\n            cur_chunk = words_keep.copy()\n            cur_words = sum(len(s.split()) for s in cur_chunk)\n        cur_chunk.append(sent)\n        cur_words += sent_words\n    # final flush\n    if cur_chunk:\n        chunks.append(\" \".join(cur_chunk).strip())\n    return chunks","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T17:48:42.273652Z","iopub.execute_input":"2025-10-30T17:48:42.274024Z","iopub.status.idle":"2025-10-30T17:48:42.281779Z","shell.execute_reply.started":"2025-10-30T17:48:42.273998Z","shell.execute_reply":"2025-10-30T17:48:42.280810Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"# ============================\n# 5) Build embeddings & FAISS (cosine / inner-product) with caching\n# ============================\nimport numpy as np\n\ndef build_or_load_index(chunks: List[str], emb_model_name: str = EMB_MODEL_NAME):\n    # If cached files exist, load them\n    if os.path.exists(CHUNKS_CACHE_FNAME) and os.path.exists(EMB_CACHE_FNAME) and os.path.exists(FAISS_INDEX_FNAME):\n        print(\"Loading cached chunks, embeddings and FAISS index from disk...\")\n        with open(CHUNKS_CACHE_FNAME, \"r\", encoding=\"utf8\") as f:\n            cached_chunks = json.load(f)\n        embeddings = np.load(EMB_CACHE_FNAME)\n        index = faiss.read_index(FAISS_INDEX_FNAME)\n        # load SBERT model (for query embedding)\n        sbert = SentenceTransformer(emb_model_name)\n        return cached_chunks, index, embeddings, sbert\n\n    # Build afresh\n    print(\"Loading SBERT model for embeddings:\", emb_model_name)\n    sbert = SentenceTransformer(emb_model_name)\n    print(\"Encoding chunks (this can take a while)...\")\n    embeddings = sbert.encode(chunks, convert_to_numpy=True, show_progress_bar=True)\n    # normalize embeddings for cosine similarity (IndexFlatIP)\n    faiss.normalize_L2(embeddings)\n    dim = embeddings.shape[1]\n    index = faiss.IndexFlatIP(dim)\n    index.add(embeddings)\n    # persist\n    print(\"Caching chunks, embeddings and FAISS index to disk...\")\n    with open(CHUNKS_CACHE_FNAME, \"w\", encoding=\"utf8\") as f:\n        json.dump(chunks, f)\n    np.save(EMB_CACHE_FNAME, embeddings)\n    faiss.write_index(index, FAISS_INDEX_FNAME)\n    return chunks, index, embeddings, sbert\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T17:31:33.492642Z","iopub.execute_input":"2025-10-30T17:31:33.493018Z","iopub.status.idle":"2025-10-30T17:31:33.501090Z","shell.execute_reply.started":"2025-10-30T17:31:33.492990Z","shell.execute_reply":"2025-10-30T17:31:33.500028Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"def load_generator_model(model_name: str = HF_GEN_MODEL, lora_path: str = None):\n    print(f\"ğŸ” Loading base model: {model_name}\")\n    dtype = torch.float16 if DEVICE == \"cuda\" else torch.float32\n\n    # Load tokenizer\n    tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False, trust_remote_code=True)\n\n    # Load base model on a single GPU (force cuda:0)\n    model = AutoModelForCausalLM.from_pretrained(\n        model_name,\n        torch_dtype=dtype,\n        low_cpu_mem_usage=True,\n                # âš ï¸ turn off auto device mapping\n        trust_remote_code=True\n    ).to(DEVICE)                # âœ… explicitly move to one device\n\n    # If LoRA fine-tuned weights are provided\n    if lora_path:\n        \n        model = PeftModel.from_pretrained(model, lora_path)\n        model.to(DEVICE)        # âœ… ensure LoRA adapter is also on same device\n    \n        \n\n    model.eval()\n    \n    return tokenizer, model\n\n \ndef generate_with_context(question: str, retrieved_chunks: List[str], tokenizer, model,\n                          max_new_tokens: int = 512, temperature: float = 0.0, top_p: float = 0.95):\n    \"\"\"\n    Assembles the prompt and generates an answer using the LLM.\n    The prompt instructs the model to answer only from provided context and to say 'Not found.' if not present.\n    \"\"\"\n    # Add short source labels to each chunk for traceability\n    labeled = []\n    for i, c in enumerate(retrieved_chunks, start=1):\n        snippet = c if len(c) < 1000 else c[:1000] + \" ...[truncated]\"\n        labeled.append(f\"[DOC {i}]\\n{snippet}\")\n    context = \"\\n\\n\".join(labeled)\n\n    prompt = (\n        \"You are an assistant that answers questions using ONLY the provided document excerpts.\\n\"\n        \"If the answer is not present in the excerpts, reply exactly: Not found.\\n\"\n        \"Cite the source by DOC number if providing facts.\\n\\n\"\n        f\"CONTEXT:\\n{context}\\n\\nQUESTION:\\n{question}\\n\\nANSWER:\"\n    )\n\n    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=4096).to(DEVICE)\n    gen_cfg = GenerationConfig(max_new_tokens=max_new_tokens, temperature=temperature, top_p=top_p)\n    with torch.no_grad():\n        outputs = model.generate(**inputs, max_new_tokens=gen_cfg.max_new_tokens,\n                                 do_sample=(temperature > 0.0), temperature=gen_cfg.temperature,\n                                 top_p=gen_cfg.top_p)\n    generated = tokenizer.decode(outputs[0][inputs[\"input_ids\"].shape[1]:], skip_special_tokens=True).strip()\n    return generated","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T17:42:34.890917Z","iopub.execute_input":"2025-10-30T17:42:34.891819Z","iopub.status.idle":"2025-10-30T17:42:34.902192Z","shell.execute_reply.started":"2025-10-30T17:42:34.891791Z","shell.execute_reply":"2025-10-30T17:42:34.901276Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"# ============================\n# 7) Full pipeline run: extract -> chunk -> index -> interactive Q&A\n# ============================\n# 7.1 Extract text\nprint(\"Extracting text from PDF (this may take some time for large PDFs)...\")\nfull_text = extract_text_from_pdf(PDF_INPUT_PATH)\nprint(\"Extracted text length (chars):\", len(full_text))\n\n# 7.2 Chunk text\nprint(\"Chunking text into sentence-based chunks...\")\nchunks = chunk_text(full_text, chunk_size_words=CHUNK_SIZE_WORDS, overlap_words=CHUNK_OVERLAP_WORDS)\nprint(\"Number of chunks:\", len(chunks))\n\n# 7.3 Build or load FAISS\nchunks, faiss_index, embeddings, emb_model = build_or_load_index(chunks, EMB_MODEL_NAME)\nprint(\"FAISS index has\", faiss_index.ntotal, \"vectors.\")\n\n# 7.4 Load generator\ntokenizer, generator_model = load_generator_model(HF_GEN_MODEL,\"/kaggle/working/deepseek-1.5b-physics-lora\")\n\n# 7.5 Interactive loop (you can also script queries)\ndef answer_query(question: str, top_k: int = TOP_K):\n    # embed question\n    q_emb = emb_model.encode([question], convert_to_numpy=True)\n    faiss.normalize_L2(q_emb)\n    D, I = faiss_index.search(q_emb, top_k)\n    retrieved = [chunks[i] for i in I[0] if i != -1]\n    # show retrieved chunk indices & scores (D are similarity scores)\n    print(\"\\n--- Retrieved (top_k={}): ---\".format(top_k))\n    for rank, (idx, score) in enumerate(zip(I[0], D[0]), start=1):\n        if idx == -1: continue\n        print(f\"Rank {rank} â€” chunk_id={idx} â€” score={score:.4f} â€” preview: {repr(chunks[idx][:200])}\")\n\n    # generate\n    ans = generate_with_context(question, retrieved, tokenizer, generator_model)\n    return ans\n\n# Example query (change as needed)\nexample_q = \"What happens on increasing magnetic fields?\"\nprint(\"\\nExample query:\", example_q)\nprint(\"Answer:\\n\", answer_query(example_q, top_k=TOP_K))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T18:09:35.123017Z","iopub.execute_input":"2025-10-30T18:09:35.123301Z","iopub.status.idle":"2025-10-30T18:10:04.665150Z","shell.execute_reply.started":"2025-10-30T18:09:35.123280Z","shell.execute_reply":"2025-10-30T18:10:04.664392Z"}},"outputs":[{"name":"stdout","text":"Extracting text from PDF (this may take some time for large PDFs)...\nExtracted text length (chars): 34903\nChunking text into sentence-based chunks...\nNumber of chunks: 18\nLoading cached chunks, embeddings and FAISS index from disk...\nFAISS index has 18 vectors.\nğŸ” Loading base model: deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\n\nExample query: What happens on increasing magnetic fields?\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9565bd405a604f779f526b3d233b58bd"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:601: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:606: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:601: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:606: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n  warnings.warn(\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"\n--- Retrieved (top_k=5): ---\nRank 1 â€” chunk_id=9 â€” score=0.2473 â€” preview: 'In this scheme the enhancement of the rcp component and consequently, the band asymmetry is exaggerated in order to\\nfacilitate the understanding of the effect. Angewandte\\nChemie\\nCommunications\\nAngew. '\nRank 2 â€” chunk_id=7 â€” score=0.2247 â€” preview: 'â€”_â€”_ Â°G,\\n205 â€” tas *Ke\\nâ€”<â€”<â€”<â€” Â°G,\\nâ€”â€”_ 3H,\\n254\\nâ€” 5G,\\nâ€” Â°F,\\n5\\nâ€”_â€” Â°F,\\n20, Â°F\\nil Â°F, 5S,\\n154 â€”â€” FFs\\n14\\no4 â€” 5\\n\\n700\\n\\n[PAGE 4]\\nconsiderable intensities (three positive and three negative)\\npossibilities th'\nRank 3 â€” chunk_id=11 â€” score=0.2240 â€” preview: 'Variable-temperature MCD spectra for HoIII complex under a 0.25 T applied magnetic field. (a) VT MCD spectra for HoIII complex. (b)\\nTemperature dependence of the different Î” parameters. (c) Relative s'\nRank 4 â€” chunk_id=6 â€” score=0.1934 â€” preview: 'Besides the outstanding results obtained with the NdIII\\nand DyIII complexes, we also studied the magneto-optical\\nthermometric performance of the HoIII complex (Figure 3). For this complex, the presenc'\nRank 5 â€” chunk_id=8 â€” score=0.1835 â€” preview: 'Nevertheless, upon the application of a\\n5.00 T magnetic field, discernible Î´T values well beyond 1 K\\ncome into play. This observation indicates the unsuitability\\nof the presented approach for precise '\nAnswer:\n [ ]\nFrom the context provided, I need to determine what happens to the temperature when the magnetic field increases. The options are not provided, but the user is asking for the effect of increasing the magnetic field on temperature.\n\nLooking at the provided documents, particularly the ones related to magneto-optical thermometry (MCD), I can find relevant information.\n\nIn [DOC 2], it mentions that increasing the magnetic field strength decreases the temperature sensitivity. It also states that increasing the magnetic field results in a broader opening of the Zeeman sublevel gaps, enhancing the energy required to populate the upper sublevels. This effect is observed in the HoIII complex, where Sm values are calculated as high as 42.3%K at 2.50 K with a 0.25 T applied magnetic field.\n\nAdditionally, in [DOC 5], it's mentioned that at higher magnetic fields, Î´T values exceed 1 K, indicating that the approach's precision is compromised. However, the document also discusses the use of multiple thermometric parameters and a multiple regression method to improve the accuracy of temperature measurement. This suggests that while the initial approach may have limitations, advancements can be made by combining multiple parameters.\n\nPutting this together, increasing the magnetic field leads to a decrease in temperature sensitivity and a broader Zeeman gap, which are both effects that can be observed in the context of magneto-optical thermometry.\n</think>\n\nIncreasing the magnetic field strength leads to a decrease in temperature sensitivity and a broader opening of the Zeeman sublevel gaps, as observed in the HoIII complex. This effect is evident in the calculations of Sm values and the corresponding magnetic field requirements. Additionally, while the initial approach may have limitations at higher magnetic fields, advancements such as the use of multiple thermometric parameters and a multiple regression method can enhance the accuracy of temperature measurement. \n\nANSWER: Decrease in temperature sensitivity and broader Zeeman gaps\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch, gc\ngc.collect()\ntorch.cuda.empty_cache()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T08:23:05.213056Z","iopub.execute_input":"2025-10-30T08:23:05.213852Z","iopub.status.idle":"2025-10-30T08:23:05.726478Z","shell.execute_reply.started":"2025-10-30T08:23:05.213826Z","shell.execute_reply":"2025-10-30T08:23:05.725864Z"}},"outputs":[],"execution_count":21}]}